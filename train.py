# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªä½¿ç”¨Hugging Face TRLåº“å¯¹Gemmaæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒçš„å®Œæ•´è„šæœ¬ã€‚
æµç¨‹åŒ…æ‹¬ï¼šç¯å¢ƒè®¾ç½®ã€æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†éªŒè¯ä»¥åŠæ¨¡å‹åˆå¹¶ã€‚
åªéœ€ä¿®æ”¹MODEL_IDï¼Œå…¶ä»–è·¯å¾„è‡ªåŠ¨ç”Ÿæˆã€‚

å®‰è£…ä¾èµ–ï¼š
pip install pytorch,bitsandbytes,datasets,huggingface-hub,peft,pillow,torch,transformers,trl
    
"""

import os
import json
import shutil
import torch
from datasets import load_dataset, Dataset
from huggingface_hub import login
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from trl import SFTTrainer, SFTConfig
import logging

# --- å…¨å±€é…ç½® ---
# è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œæ–¹ä¾¿è°ƒè¯•
logging.basicConfig(level=logging.INFO)

# --- ç¬¬1éƒ¨åˆ†: ç¯å¢ƒè®¾ç½®ä¸æ¨¡å‹åŠ è½½ ---
logging.info("--- Part 1: ç¯å¢ƒè®¾ç½®ä¸æ¨¡å‹åŠ è½½ ---")

# åŸºç¡€ç›®å½•è®¾ç½®
BASE_DIR = "."

# æ•°æ®é›†æ–‡ä»¶
DATASET_FILE = "dataset.jsonl"

# å®šä¹‰æ¨¡å‹ID - åªéœ€è¦ä¿®æ”¹è¿™ä¸€è¡Œï¼
MODEL_ID = "google/gemma-3-270m-it"

# å®šä¹‰è®¾å¤‡ï¼ˆcpu: None, gpu: "cuda"ï¼‰
DEVICE = None

# è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹ç›¸å…³çš„è·¯å¾„å’Œåç§°
def generate_paths_from_model_id(model_id):
    """æ ¹æ®model_idè‡ªåŠ¨ç”Ÿæˆç›¸å…³è·¯å¾„"""
    # æå–æ¨¡å‹åç§°ï¼Œå»æ‰ç»„ç»‡å‰ç¼€ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦
    model_name = model_id.split('/')[-1].replace('-', '_')
    
    ollama_name = f"{model_name}_lora_merged"
    adapter_path = os.path.join(BASE_DIR, f"{model_name}_lora_adapter")
    merged_path = os.path.join(BASE_DIR, f"{model_name}_lora_merged")
    
    return ollama_name, adapter_path, merged_path

OLLAMA_MODEL_NAME, ADAPTER_PATH, MERGED_MODEL_PATH = generate_paths_from_model_id(MODEL_ID)

logging.info(f"æ¨¡å‹ID: {MODEL_ID}")
logging.info(f"Ollamaæ¨¡å‹å: {OLLAMA_MODEL_NAME}")
logging.info(f"é€‚é…å™¨è·¯å¾„: {ADAPTER_PATH}")
logging.info(f"åˆå¹¶æ¨¡å‹è·¯å¾„: {MERGED_MODEL_PATH}")

# ç™»å½•Hugging Face (æ¨èä½¿ç”¨huggingface-cli loginå‘½ä»¤ï¼Œè€Œä¸æ˜¯åœ¨ä»£ç ä¸­ç¡¬ç¼–ç token)
# å¦‚æœç¯å¢ƒä¸­å·²ç»è®¾ç½®äº†HF_TOKENï¼Œåˆ™æ— éœ€æ­¤è¡Œ
# login(token='YOUR_HF_TOKEN')

# åŠ è½½åˆ†è¯å™¨ (Tokenizer)
logging.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{MODEL_ID}' çš„åˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
# Gemmaæ¨¡å‹å¯èƒ½æ²¡æœ‰é»˜è®¤çš„å¡«å……ä»¤ç‰Œï¼Œéœ€è¦æ‰‹åŠ¨è®¾ç½®
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# é…ç½®é‡åŒ–å‚æ•°
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # å¯ç”¨4ä½é‡åŒ–
    bnb_4bit_use_double_quant=True,       # æ˜¯å¦ä½¿ç”¨åŒé‡é‡åŒ–
    bnb_4bit_quant_type="nf4",             # é‡åŒ–ç±»å‹ï¼Œnf4æ˜¯ä¸€ç§å½’ä¸€åŒ–æµ®ç‚¹4ä½æ ¼å¼
    bnb_4bit_compute_dtype=torch.float16 # è®¡ç®—æ—¶ä½¿ç”¨çš„dtypeï¼ŒåŠ é€Ÿä¸”ç²¾åº¦é€‚ä¸­
)

# åŠ è½½åŸºç¡€æ¨¡å‹
logging.info(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ '{MODEL_ID}'...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,  # ä¼ å…¥é‡åŒ–é…ç½®
    device_map=DEVICE,  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ (CPU/GPU)
    dtype=torch.float16, # ä½¿ç”¨bfloat16ä»¥èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿ
    trust_remote_code=True,
    low_cpu_mem_usage=True  # å¯é€‰ï¼Œå‡å°CPUå†…å­˜å ç”¨
)

# å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒ
model = prepare_model_for_kbit_training(model)

logging.info(f"âœ… {MODEL_ID} æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸåŠ è½½ï¼")
logging.info(f"æ¨¡å‹æ˜¾å­˜å ç”¨: {model.get_memory_footprint() / 1e9:.2f} GB")


# --- ç¬¬2éƒ¨åˆ†: è®­ç»ƒæ•°æ®å‡†å¤‡ ---
logging.info("\n--- Part 2: è®­ç»ƒæ•°æ®å‡†å¤‡ ---")

# Alpacaæ ¼å¼çš„è®­ç»ƒæ ·æœ¬
training_examples = [
    {"messages": [
        {"role": "user", "content": "åˆ†æä¸€ä¸‹ç¾å›¢æœ€æ–°çš„è´¢æŠ¥ï¼Œé‡ç‚¹çœ‹å¤–å–ä¸šåŠ¡çš„å¢é•¿é©±åŠ¨åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"role": "assistant", "content": "æ ¹æ®æœ€æ–°çš„è´¢æŠ¥ï¼Œç¾å›¢å¤–å–ä¸šåŠ¡çš„å¢é•¿ä¸»è¦ç”±è®¢å•é¢‘ç‡çš„å¢åŠ å’Œå®¢å•ä»·çš„å¾®å‡å…±åŒé©±åŠ¨ã€‚å°¤å…¶æ˜¯åœ¨ä¸‰çº¿åŠä»¥ä¸‹åŸå¸‚ï¼Œç”¨æˆ·æ¸—é€ç‡çš„æå‡å¸¦æ¥äº†æ˜¾è‘—çš„è®¢å•é‡å¢é•¿ã€‚"}
    ]},
    {"messages": [
        {"role": "user", "content": "ä»Šå¤©æ¸¯è‚¡å¤§è·Œï¼Œç¾å›¢çš„è‚¡ä»·è¡¨ç°å¦‚ä½•ï¼Œæœ‰ä»€ä¹ˆéœ€è¦å…³æ³¨çš„æŠ€æœ¯æŒ‡æ ‡å—ï¼Ÿ"},
        {"role": "assistant", "content": "åœ¨ä»Šå¤©æ’ç”ŸæŒ‡æ•°æ•´ä½“ä¸‹æŒ«2.5%çš„èƒŒæ™¯ä¸‹ï¼Œç¾å›¢è‚¡ä»·ä¸‹è·Œäº†4.1%ï¼Œé£é™©æ˜¾è‘—æ”¾å¤§ã€‚è™½ç„¶RSIæŒ‡æ ‡å·²è¿›å…¥è¶…å–åŒºé—´ï¼Œå¯èƒ½å­˜åœ¨æŠ€æœ¯æ€§åå¼¹ï¼Œä½†æ›´åº”è­¦æƒ•çš„æ˜¯ï¼Œè‚¡ä»·å·²è·Œç ´120æ¸¯å…ƒè¿™ä¸€å…³é”®é•¿æœŸæ”¯æ’‘ä½ï¼Œè‹¥æ— æ³•è¿…é€Ÿæ”¶å¤ï¼Œåˆ™ä¸‹é™é€šé“å¯èƒ½è¢«è¿›ä¸€æ­¥ç¡®è®¤ï¼Œåç»­ä»æœ‰ä¸‹è¡Œå‹åŠ›ã€‚"}
    ]}
]

# å°†æ ·æœ¬é€è¡Œå†™å…¥JSONLæ–‡ä»¶
with open(DATASET_FILE, 'w', encoding='utf-8') as f:
    for example in training_examples:
        f.write(json.dumps(example, ensure_ascii=False) + '\n')

print(f"æ–‡æœ¬æ•°æ®æ–‡ä»¶ '{DATASET_FILE}' åˆ›å»ºæˆåŠŸï¼")

# ä½¿ç”¨datasetsåº“åŠ è½½æ•°æ®
text_dataset = load_dataset("json", data_files=DATASET_FILE, split="train")
print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
print("æ•°æ®é›†å†…å®¹é¢„è§ˆ:", text_dataset[0])


# --- ç¬¬3éƒ¨åˆ†: æ ¸å¿ƒå¾®è°ƒæµç¨‹ ---
logging.info("\n--- Part 3: æ ¸å¿ƒå¾®è°ƒæµç¨‹ ---")

# 1. é…ç½®LoRA
lora_config = LoraConfig(
    r=16,                      # LoRAçš„ç§©ï¼Œå¢åŠ åˆ°16ä»¥è·å¾—å¯èƒ½æ›´å¥½çš„æ€§èƒ½
    lora_alpha=32,             # é€šå¸¸è®¾ä¸ºrçš„ä¸¤å€
    lora_dropout=0.05,         # Dropoutæ¯”ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules="all-linear",# å°†LoRAåº”ç”¨åˆ°æ‰€æœ‰çº¿æ€§å±‚
    task_type="CAUSAL_LM",     # ä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€æ¨¡å‹
)

# 2. é…ç½®è®­ç»ƒå‚æ•°
training_args = SFTConfig( # <--- å…³é”®ä¿®å¤ï¼šä½¿ç”¨ SFTConfig
    output_dir=os.path.join(BASE_DIR, "training_checkpoints"), # è®­ç»ƒæ£€æŸ¥ç‚¹è¾“å‡ºç›®å½•
    per_device_train_batch_size=2, # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 2 * 4 = 8
    learning_rate=2e-4,            # å­¦ä¹ ç‡
    num_train_epochs=3,            # è®­ç»ƒè½®æ¬¡
    logging_steps=1,               # æ¯1æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    # optim="paged_adamw_8bit",      # ä½¿ç”¨8ä½åˆ†é¡µä¼˜åŒ–å™¨ä»¥èŠ‚çœæ˜¾å­˜
    bf16=False,                    # å¯ç”¨BF16æ··åˆç²¾åº¦è®­ç»ƒ (å¦‚æœGPUæ”¯æŒ)
    report_to="none",              # å…³é—­å¤–éƒ¨æŠ¥å‘Šå·¥å…· (å¦‚wandb)
    max_length=1024,           # <--- æ­£ç¡®ä½ç½®ï¼šåœ¨è¿™é‡Œè®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
)

# 3. åˆ›å»ºSFTTrainerå®ä¾‹
trainer = SFTTrainer(
    model=model,
    train_dataset=text_dataset,
    peft_config=lora_config,
    args=training_args,
    processing_class=tokenizer, # <--- å…³é”®ä¿®å¤ï¼šä½¿ç”¨ 'processing_class' æ›¿ä»£ 'tokenizer'
)

# 4. å¼€å§‹è®­ç»ƒ
logging.info("ğŸš€ å¼€å§‹å¾®è°ƒ...")
trainer.train()
logging.info("ğŸ‰ å¾®è°ƒå®Œæˆï¼")

# 5. ä¿å­˜è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨
trainer.save_model(ADAPTER_PATH)
logging.info(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜è‡³: {ADAPTER_PATH}")


# --- ç¬¬4éƒ¨åˆ†: æ¨ç†ä¸éªŒè¯ ---
logging.info("\n--- Part 4: æ¨ç†ä¸éªŒè¯ ---")

# æ¸…ç†GPUç¼“å­˜
del trainer
del model
if DEVICE == "cuda":
    torch.cuda.empty_cache()

# 1. é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨è¿›è¡Œæ¨ç†
logging.info("ä¸ºæ¨ç†é‡æ–°åŠ è½½æ¨¡å‹å’Œé€‚é…å™¨...")
base_model_for_inference = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,  # ä¼ å…¥é‡åŒ–é…ç½®
    device_map=DEVICE,
    dtype=torch.float16,
    trust_remote_code=True,
    low_cpu_mem_usage=True  # å¯é€‰ï¼Œå‡å°CPUå†…å­˜å ç”¨
)

# å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒ
base_model_for_inference = prepare_model_for_kbit_training(base_model_for_inference)

model_with_adapter = PeftModel.from_pretrained(base_model_for_inference, ADAPTER_PATH)
logging.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼Œæ¨¡å‹å·²å‡†å¤‡å¥½å¯¹è¯ï¼")


def chat_with_finetuned_model(question: str):
    """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œå•è½®å¯¹è¯çš„å‡½æ•°"""
    logging.info("\næ¨¡å‹æ­£åœ¨ç”Ÿæˆå›ç­”...")
    
    # æ„å»ºä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„èŠå¤©æ¨¡æ¿
    messages = [{"role": "user", "content": question}]

    # åº”ç”¨æ¨¡æ¿å¹¶è¿›è¡Œç¼–ç 
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model_with_adapter.device)

    # ç”Ÿæˆå›ç­”
    outputs = model_with_adapter.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        top_p=0.9,
        temperature=0.7
    )

    # è§£ç å¹¶æ‰“å°ç»“æœ
    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    print("\n--- æ¨¡å‹å›ç­” ---")
    print(response_text)
    print("------------------")

# 2. å¼€å§‹æµ‹è¯•
chat_with_finetuned_model("åˆ†æä¸€ä¸‹è¿‘æœŸæ’ç”Ÿç§‘æŠ€æŒ‡æ•°çš„è¶‹åŠ¿")
chat_with_finetuned_model("ç¾å›¢æœ€æ–°çš„è´¢æŠ¥æ€ä¹ˆæ ·ï¼Ÿ")


# --- ç¬¬5éƒ¨åˆ†: åˆå¹¶å¹¶ä¿å­˜æ¨¡å‹ ---
logging.info("\n--- Part 5: åˆå¹¶å¹¶ä¿å­˜æ¨¡å‹ ---")

# 1. åˆå¹¶é€‚é…å™¨æƒé‡åˆ°åŸºç¡€æ¨¡å‹
logging.info("æ­£åœ¨åˆå¹¶LoRAé€‚é…å™¨...")
merged_model = model_with_adapter.merge_and_unload()
logging.info("âœ… æ¨¡å‹åˆå¹¶å®Œæˆ")

# 2. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹å’Œåˆ†è¯å™¨
logging.info(f"æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹è‡³: {MERGED_MODEL_PATH}")
merged_model.save_pretrained(MERGED_MODEL_PATH)
tokenizer.save_pretrained(MERGED_MODEL_PATH)
logging.info("âœ… åˆå¹¶åçš„æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸä¿å­˜ï¼")

exit()
# --- (å¯é€‰) ç¬¬6éƒ¨åˆ†: åˆ›å»ºOllamaæ¨¡å‹æ–‡ä»¶ ---
logging.info("\n--- (Optional) Part 6: åˆ›å»ºOllamaæ¨¡å‹æ–‡ä»¶ ---")

try:
    # æ£€æŸ¥åŸå§‹tokenizer.modelæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    # æ³¨æ„ï¼šè¿™éƒ¨åˆ†ä¾èµ–äºHugging Faceçš„ç¼“å­˜ç»“æ„ï¼Œå¯èƒ½ä¸ç¨³å®š
    hub_cache_path = os.path.expanduser("~/.cache/huggingface/hub")
    snapshot_path_pattern = os.path.join(hub_cache_path, f"models--{MODEL_ID.replace('/', '--')}", "snapshots")
    
    if os.path.exists(snapshot_path_pattern):
        # è·å–æœ€æ–°çš„snapshot
        latest_snapshot = sorted(os.listdir(snapshot_path_pattern))[-1]
        source_tokenizer_model = os.path.join(snapshot_path_pattern, latest_snapshot, "tokenizer.model")
        
        if os.path.exists(source_tokenizer_model):
            destination_tokenizer_model = os.path.join(MERGED_MODEL_PATH, "tokenizer.model")
            shutil.copy(source_tokenizer_model, destination_tokenizer_model)
            logging.info(f"å·²ä»ç¼“å­˜å¤åˆ¶ tokenizer.model åˆ° {MERGED_MODEL_PATH}")

            # åˆ›å»ºModelfile
            modelfile_content = f"FROM {MERGED_MODEL_PATH}"
            modelfile_path = os.path.join(BASE_DIR, "Modelfile")
            with open(modelfile_path, "w") as f:
                f.write(modelfile_content)
            
            # åˆ›å»ºå¹¶è¿è¡ŒOllamaæ¨¡å‹
            logging.info(f"æ­£åœ¨åˆ›å»ºOllamaæ¨¡å‹: {OLLAMA_MODEL_NAME}")
            os.system(f"ollama create {OLLAMA_MODEL_NAME} -f {modelfile_path}")
            logging.info(f"âœ… Ollamaæ¨¡å‹åˆ›å»ºæˆåŠŸï¼è¯·ä½¿ç”¨ `ollama run {OLLAMA_MODEL_NAME}` è¿è¡Œã€‚")
        else:
            logging.warning("åœ¨Hugging Faceç¼“å­˜ä¸­æœªæ‰¾åˆ° tokenizer.modelï¼Œè·³è¿‡Ollamaåˆ›å»ºã€‚")
    else:
        logging.warning("æœªæ‰¾åˆ°Hugging Faceæ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œè·³è¿‡Ollamaåˆ›å»ºã€‚")

except Exception as e:
    logging.error(f"åˆ›å»ºOllamaæ¨¡å‹æ—¶å‡ºé”™: {e}")

logging.info(f"\nğŸ‰ å¾®è°ƒå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
logging.info(f"- LoRAé€‚é…å™¨: {ADAPTER_PATH}")
logging.info(f"- åˆå¹¶æ¨¡å‹: {MERGED_MODEL_PATH}")
logging.info(f"- Ollamaæ¨¡å‹: {OLLAMA_MODEL_NAME}")
